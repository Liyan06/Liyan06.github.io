<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>SVM, Dual SVM, Non-linear SVM | Liyan Tang</title>
<meta name="keywords" content="ML, MATH" />
<meta name="description" content="Linear SVM Idea We want to find a hyper-plane $w^\top x &#43; b = 0$ that maximizes the margin.
Set up We first show that the vector $w$ is orthogonal to this hyper-plane. Let $x_1$, $x_2$ be any element on the hyper-plane. So we have $w^\top x_1 &#43; b = 0$ and $w^\top x_2 &#43; b = 0$. Then $w^\top (x_1 - x_2) = 0$, which implies $w$ is orthogonal to the hyper-plane.">
<meta name="author" content="Theme PaperMod">
<link rel="canonical" href="https://Liyan06.github.io/posts/svm/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.35cd0f65a15cafa92372b8313deef5960aae04b90ad722f2bbf509eb0468137e.css" integrity="sha256-Nc0PZaFcr6kjcrgxPe71lgquBLkK1yLyu/UJ6wRoE34=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://Liyan06.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://Liyan06.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://Liyan06.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://Liyan06.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://Liyan06.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.85.0" />
<script>
MathJax = {
  tex: {
    inlineMath: [ ['$', '$'], ['\\(', '\\)'] ]
  }
  ,svg: {
    fontCache: 'global'
  }
};
</script>
<script
  type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-202974782-1', 'auto');
	
	ga('send', 'pageview');
}
</script><meta property="og:title" content="SVM, Dual SVM, Non-linear SVM" />
<meta property="og:description" content="Linear SVM Idea We want to find a hyper-plane $w^\top x &#43; b = 0$ that maximizes the margin.
Set up We first show that the vector $w$ is orthogonal to this hyper-plane. Let $x_1$, $x_2$ be any element on the hyper-plane. So we have $w^\top x_1 &#43; b = 0$ and $w^\top x_2 &#43; b = 0$. Then $w^\top (x_1 - x_2) = 0$, which implies $w$ is orthogonal to the hyper-plane." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://Liyan06.github.io/posts/svm/" /><meta property="og:image" content="https://Liyan06.github.io/papermod-cover.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-04-01T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2020-04-01T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://Liyan06.github.io/papermod-cover.png"/>

<meta name="twitter:title" content="SVM, Dual SVM, Non-linear SVM"/>
<meta name="twitter:description" content="Linear SVM Idea We want to find a hyper-plane $w^\top x &#43; b = 0$ that maximizes the margin.
Set up We first show that the vector $w$ is orthogonal to this hyper-plane. Let $x_1$, $x_2$ be any element on the hyper-plane. So we have $w^\top x_1 &#43; b = 0$ and $w^\top x_2 &#43; b = 0$. Then $w^\top (x_1 - x_2) = 0$, which implies $w$ is orthogonal to the hyper-plane."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://Liyan06.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "SVM, Dual SVM, Non-linear SVM",
      "item": "https://Liyan06.github.io/posts/svm/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "SVM, Dual SVM, Non-linear SVM",
  "name": "SVM, Dual SVM, Non-linear SVM",
  "description": "Linear SVM Idea We want to find a hyper-plane $w^\\top x + b = 0$ that maximizes the margin.\nSet up We first show that the vector $w$ is orthogonal to this hyper-plane. Let $x_1$, $x_2$ be any element on the hyper-plane. So we have $w^\\top x_1 + b = 0$ and $w^\\top x_2 + b = 0$. Then $w^\\top (x_1 - x_2) = 0$, which implies $w$ is orthogonal to the hyper-plane.",
  "keywords": [
    "ML", "MATH"
  ],
  "articleBody": "Linear SVM Idea We want to find a hyper-plane $w^\\top x + b = 0$ that maximizes the margin.\nSet up We first show that the vector $w$ is orthogonal to this hyper-plane. Let $x_1$, $x_2$ be any element on the hyper-plane. So we have $w^\\top x_1 + b = 0$ and $w^\\top x_2 + b = 0$. Then $w^\\top (x_1 - x_2) = 0$, which implies $w$ is orthogonal to the hyper-plane. Now, we set two dashed lines to $w^\\top x + b = 1$ and $w^\\top x + b = -1$. In fact, “$1$” doesn’t matter and we can pick any value here. “$1$” is just the convention.\nNow we pick any line parallel (orthogonal) to $w$ (hyper-plane), then the line intersect two dashed line with point $x^{(+)}$ and $x^{(-)}$. We want to maximize the margin\n$$margin = || x^{(+)} - x^{(-)}||$$\nRe-express margin Since $w$ is parallel to $x^{(+)} - x^{(-)}$, we have $x^{(+)} - x^{(-)}$ = $\\lambda w$ for some $\\lambda$. Then\n$$x^{(+)} = \\lambda w + x^{(-)}$$\nSince $w^\\top x^{(+)} + b = 1$, we have $w^\\top (\\lambda w + x^{(-)}) + b = 1$ and then $\\lambda w^\\top w + w^\\top x^{(-)} + b = 1$. Since $w^\\top x^{(-)} + b = -1$, we have $\\lambda = w^\\top w = 2$. So\n$$ \\lambda = \\frac{2}{w^\\top w}$$\nNow we can rewrite the margin as\n$$ margin = ||(\\lambda w + x^{(-)}) - x^{(-)}|| = ||\\lambda w|| = ||\\frac{2}{w^\\top w} w || = \\frac{2}{||w||}$$\nConstruct an optimization problem we can construct the following objective function for SVM:\nwe can re-write it as\nSoft version of linear SVM Note that the above constraints are hard constraints and it only works if the data are linearly separable.\nTherefore, if data is not linearly separable, we want to make a soft constraint (relaxation). That is, we allow the model to make some error, but we will add some penalty for them.\nNote that if $\\lambda \\to \\infty$, then we allow no error; if $\\lambda = 0$, then we add no penalty. We call $\\epsilon_i$ a slack variable. Ideally, we want $\\epsilon_i = 0$; if it makes an error, $\\epsilon_i  0$.\nConvert to Hinge Loss Since $(w^\\top x^{(i)} + b)y^{(i)} \\geq 1 - \\epsilon_i$, we have $\\epsilon_i \\geq 1-(w^\\top x^{(i)} + b)y^{(i)}$. If $\\epsilon_i \\leq 0$, we have no loss. Otherwise, we add $\\epsilon_i = 1-(w^\\top x^{(i)} + b)y^{(i)}$ as loss. So right now our new objective function is\nStochastic Gradient descent for Hinge Loss objective function:\nNon-linear SVM We are going to map all points through a non-linear function and then used SVM in this transformed space. The idea is that if the non-linear map we use maps the two sets of points such that the two sets of points can be separated by a line after the transformation, then SVM can be used in this transformed space instead of the original space.\nLet $\\phi: \\mathcal{X} \\to \\mathcal{F}$ be the non linear map described in the earlier paragraph, where $\\mathcal{X}$ is the space from which inputs points are coming from and $\\mathcal{F}$ is the transformed space. For SVM to work, we don’t need to know $\\phi$ explicitly, but only need to know the dot product of the transformed points $⟨\\phi(x_i), \\phi(x_j)⟩$. So, instead of working with $\\phi$, they can instead work with $K: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$ where $K$ takes two points as input and returns a real value that represents $⟨\\phi(x_i), \\phi(x_j)⟩$. Note that $\\phi$ exists only when $K$ is positive definite. With this, we are able to run SVM on an infinite dimensional space.\nGram Matrix: Given a set of vectors in $\\mathcal{V}$, the Gram Matrix is the matrix of all possible inner products in $\\mathcal{V}$. That is, $G_{ij} = v_i \\cdot v_j$.\nCurse of Dimensionality As the dimensionality increases, the classifier’s performance increases until the optimal number of features is reached. Further increasing the dimensionality without increasing the number of training samples results in a decrease in classifier performance. The common theme of these problems is that when the dimensionality increases, the volume of the space increases so fast that the available data become sparse. In order to obtain a statistically sound and reliable result, the amount of data needed to support the result often grows exponentially with the dimensionality. Also, the time complexity of a classification algorithm is proportional to the dimension of the data point. So, higher dimension means larger time complexity (not to mention space complexity to store those large dimensional points).\nMercer’s Theorem (simplified idea): In a finite input space, if the Kernel matrix $\\mathbf{K}$ (also known as Gram matrix) is positive semi-definite ($\\mathbf{K}_{ij} = K(x_i, x_j) = \\phi(x_i) \\cdot \\phi(x_j)$), then the matrix element, i.e. the function K, can be a kernel function.\nExample of a kernel function Let $x_i = (x_{i1}, x_{i2}), x_j = (x_{j1}, x_{j2})$ and $\\phi(x_i)=(x_{i1}^2, \\sqrt{2}x_{i1}x_{i2}, x_{i2}^2), \\phi(x_j)=(x_{j1}^2, \\sqrt{2}x_{j1}x_{j2}, x_{j2}^2)$. Then\nFrom this example, we see that even if the dimension of the feature space is higher than the input space, we can still do the computation in the low dimension input space as long as we choose a good kernel! Therefore, it’s possible to run SVM on an infinite dimensional feature space but do the same amount of computation as in the low dimension input space if we choose a good kernel.\nNote:\n Let $n$ be the dimension of input space, $N (» n)$ be the dimension of the feature space, then if we choose a kernel $K$ properly, we are able to compute the dot product in higher dimensional space but in complexity $O(n)$ instead of $O(N)$. If the classification algorithm is only dependent on the dot product and has no dependency on the actual map $\\phi$, I can use the kernel trick to run the algorithm in high dimensional space with almost no additional cost.  Common Kernel Function Polynomial Kernel $$k(x_i, x_j) = (x_i \\cdot x_j + 1)^d$$\nwhere $d$ is the degree of the polynomial. This type of kernel represents the similarity of vectors in a feature space over polynomials of the original variables. It is popular in natural language processing.\nGaussian Kernel $$k(x, y) = \\text{exp}\\left(-\\frac{|x_i - x_j|^2}{2\\sigma^2}\\right)$$\nThis type of kernel is useful when there is no prior knowledge about the data; it has good performance when there is the assumption og general smoothness of the data. It is an example of the radial basis function kernel (below). $\\sigma$ is the regularization variable that can be tuned specifically for each problem.\nGaussian Radial Basis Function (RBF) $$k(x_i, x_j) = \\text{exp}(-\\gamma |x_i - x_j|^2)$$\nfor $\\gamma  0$. The difference between this kernel and the gaussian kernel is the amount of regularization applied.\nExponential Kernel $$k(x, y) = \\text{exp}\\left(-\\frac{|x_i - x_j|}{2\\sigma^2}\\right)$$\nDual problem of SVM Our primal problem is\nNote that the primal problem of SVM is a convex problem and the constraints are convex. We know that for any convex optimization problem with differentiable objective and constraint functions, any points that satisfy the KKT conditions are primal and dual optimal, and have zero duality gap.\nWe can re-write the primal problem as\nwhere $\\phi(x_i)$ is a map of $x_i$ from input space to feature space. Now\nThen we can use the $4$-th KKT condition (gradient w.r.t. $w, b, \\epsilon_i$ is $0$):\nTherefore we have\nOur dual problem of SVM is\nNote that the maximization only depends on the dot product of $\\phi(x_i), \\phi(x_j)$. We define a function\n$$K(x_i, x_j) = \\phi(x_i) \\cdot \\phi(x_j)$$\nAll we need is the function $K$, a kernel function, which provides with the dot product of two vectors in another space and we don’t need to know the transformation into the other space.\nWe can re-write the problem as\n Reference:\n https://www.quora.com/Why-should-a-kernel-function-satisfy-Mercers-condition https://scikit-learn.org/stable/modules/svm.html#svm-kernels https://kfrankc.com/posts/2019/06/21/kernel-functions-svm# https://towardsdatascience.com/understanding-support-vector-machine-part-2-kernel-trick-mercers-theorem-e1e6848c6c4d https://www.datasciencecentral.com/profiles/blogs/about-the-curse-of-dimensionality https://en.wikipedia.org/wiki/Curse_of_dimensionality https://stats.stackexchange.com/questions/44166/kernelised-k-nearest-neighbour https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec13.pdf  ",
  "wordCount" : "1294",
  "inLanguage": "en",
  "datePublished": "2020-04-01T00:00:00Z",
  "dateModified": "2020-04-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Theme PaperMod"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://Liyan06.github.io/posts/svm/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Liyan Tang",
    "logo": {
      "@type": "ImageObject",
      "url": "https://Liyan06.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<noscript>
    <style type="text/css">
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: #1d1e20;
                --entry: #2e2e33;
                --primary: rgba(255, 255, 255, 0.84);
                --secondary: rgba(255, 255, 255, 0.56);
                --tertiary: rgba(255, 255, 255, 0.16);
                --content: rgba(255, 255, 255, 0.74);
                --hljs-bg: #2e2e33;
                --code-bg: #37383e;
                --border: #333;
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://Liyan06.github.io/" accesskey="h" title="Liyan Tang (Alt + H)">Liyan Tang</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://Liyan06.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://Liyan06.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://Liyan06.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://Liyan06.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://Liyan06.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://Liyan06.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      SVM, Dual SVM, Non-linear SVM
    </h1>
    <div class="post-meta">April 1, 2020&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Theme PaperMod
</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <div class="details">Table of Contents</div>
        </summary>
        <div class="inner"><ul>
                <li>
                    <a href="#linear-svm" aria-label="Linear SVM">Linear SVM</a><ul>
                        
                <li>
                    <a href="#idea" aria-label="Idea">Idea</a></li>
                <li>
                    <a href="#set-up" aria-label="Set up">Set up</a></li>
                <li>
                    <a href="#re-express-margin" aria-label="Re-express margin">Re-express margin</a></li>
                <li>
                    <a href="#construct-an-optimization-problem" aria-label="Construct an optimization problem">Construct an optimization problem</a></li>
                <li>
                    <a href="#soft-version-of-linear-svm" aria-label="Soft version of linear SVM">Soft version of linear SVM</a></li>
                <li>
                    <a href="#convert-to-hinge-loss" aria-label="Convert to Hinge Loss">Convert to Hinge Loss</a></li></ul>
                </li>
                <li>
                    <a href="#non-linear-svm" aria-label="Non-linear SVM">Non-linear SVM</a><ul>
                        
                <li>
                    <a href="#curse-of-dimensionality" aria-label="Curse of Dimensionality">Curse of Dimensionality</a></li>
                <li>
                    <a href="#example-of-a-kernel-function" aria-label="Example of a kernel function">Example of a kernel function</a></li></ul>
                </li>
                <li>
                    <a href="#common-kernel-function" aria-label="Common Kernel Function">Common Kernel Function</a><ul>
                        
                <li>
                    <a href="#polynomial-kernel" aria-label="Polynomial Kernel">Polynomial Kernel</a></li>
                <li>
                    <a href="#gaussian-kernel" aria-label="Gaussian Kernel">Gaussian Kernel</a></li>
                <li>
                    <a href="#gaussian-radial-basis-function-rbf" aria-label="Gaussian Radial Basis Function (RBF)">Gaussian Radial Basis Function (RBF)</a></li>
                <li>
                    <a href="#exponential-kernel" aria-label="Exponential Kernel">Exponential Kernel</a></li></ul>
                </li>
                <li>
                    <a href="#dual-problem-of-svm" aria-label="Dual problem of SVM">Dual problem of SVM</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="linear-svm">Linear SVM<a hidden class="anchor" aria-hidden="true" href="#linear-svm">#</a></h2>
<img src="https://img-blog.csdnimg.cn/20200328002655944.jpg" width="600">
<h3 id="idea">Idea<a hidden class="anchor" aria-hidden="true" href="#idea">#</a></h3>
<p>We want to find a hyper-plane $w^\top x + b = 0$ that maximizes the margin.</p>
<h3 id="set-up">Set up<a hidden class="anchor" aria-hidden="true" href="#set-up">#</a></h3>
<p>We first show that the vector $w$ is orthogonal to this hyper-plane. Let $x_1$, $x_2$ be any element on the hyper-plane. So we have $w^\top x_1 + b = 0$ and $w^\top x_2 + b = 0$. Then $w^\top (x_1 - x_2) = 0$, which implies $w$ is orthogonal to the hyper-plane. Now, we set two dashed lines to $w^\top x + b = 1$ and $w^\top x + b = -1$. In fact, &ldquo;$1$&rdquo; doesn&rsquo;t matter and we can pick any value here. &ldquo;$1$&rdquo; is just the convention.</p>
<p>Now we pick any line parallel (orthogonal) to $w$ (hyper-plane), then the line intersect two dashed line with point $x^{(+)}$ and $x^{(-)}$. We want to maximize the margin</p>
<p>$$margin = || x^{(+)} - x^{(-)}||$$</p>
<h3 id="re-express-margin">Re-express margin<a hidden class="anchor" aria-hidden="true" href="#re-express-margin">#</a></h3>
<p>Since $w$ is parallel to $x^{(+)} - x^{(-)}$, we have $x^{(+)} - x^{(-)}$ = $\lambda w$ for some $\lambda$. Then</p>
<p>$$x^{(+)} = \lambda w + x^{(-)}$$</p>
<p>Since $w^\top x^{(+)} + b = 1$, we have $w^\top (\lambda w + x^{(-)}) + b = 1$ and then $\lambda w^\top w + w^\top x^{(-)} + b = 1$. Since $w^\top x^{(-)} + b = -1$, we have $\lambda = w^\top w = 2$. So</p>
<p>$$ \lambda = \frac{2}{w^\top w}$$</p>
<p>Now we can rewrite the margin as</p>
<p>$$ margin = ||(\lambda w + x^{(-)}) -  x^{(-)}|| = ||\lambda w|| = ||\frac{2}{w^\top w} w || = \frac{2}{||w||}$$</p>
<h3 id="construct-an-optimization-problem">Construct an optimization problem<a hidden class="anchor" aria-hidden="true" href="#construct-an-optimization-problem">#</a></h3>
<p>we can construct the following objective function for SVM:</p>
<img src="https://img-blog.csdnimg.cn/20200328002800822.png" width="400">
<p>we can re-write it as</p>
<img src="https://img-blog.csdnimg.cn/20200328002820917.png" width="400">
<h3 id="soft-version-of-linear-svm">Soft version of linear SVM<a hidden class="anchor" aria-hidden="true" href="#soft-version-of-linear-svm">#</a></h3>
<p>Note that the above constraints are <em>hard constraints</em> and it only works if the data are linearly separable.</p>
<p>Therefore, if data is not linearly separable, we want to make a soft constraint (relaxation). That is, we allow the model to make some error, but we will add some penalty for them.</p>
<img src="https://img-blog.csdnimg.cn/20200328002840841.png" width="500">
<p>Note that if $\lambda \to \infty$, then we allow no error; if $\lambda = 0$, then we add no penalty. We call $\epsilon_i$ a slack variable. Ideally, we want $\epsilon_i = 0$; if it makes an error, $\epsilon_i &gt; 0$.</p>
<h3 id="convert-to-hinge-loss">Convert to Hinge Loss<a hidden class="anchor" aria-hidden="true" href="#convert-to-hinge-loss">#</a></h3>
<p>Since $(w^\top x^{(i)} + b)y^{(i)} \geq 1 - \epsilon_i$, we have $\epsilon_i \geq 1-(w^\top x^{(i)} + b)y^{(i)}$. If $\epsilon_i \leq 0$, we have no loss. Otherwise, we add $\epsilon_i = 1-(w^\top x^{(i)} + b)y^{(i)}$ as loss. So right now our new objective function is</p>
<img src="https://img-blog.csdnimg.cn/20200328002913718.png" width="500">
<p><strong>Stochastic Gradient descent for Hinge Loss objective function</strong>:</p>
<img src="https://img-blog.csdnimg.cn/20200328003339521.jpeg" width="500">
<h2 id="non-linear-svm">Non-linear SVM<a hidden class="anchor" aria-hidden="true" href="#non-linear-svm">#</a></h2>
<p>We are going to map all points through a non-linear function and then used SVM in this transformed space. The idea is that if the non-linear map we use maps the two sets of points such that the two sets of points can be separated by a line after the transformation, then SVM can be used in this transformed space instead of the original space.</p>
<p>Let $\phi: \mathcal{X} \to \mathcal{F}$ be the non linear map described in the earlier paragraph, where $\mathcal{X}$ is the space from which inputs points are coming from and $\mathcal{F}$ is the transformed space. For SVM to work, we don&rsquo;t need to know $\phi$ explicitly, but only need to know the dot product of the transformed points $⟨\phi(x_i), \phi(x_j)⟩$. So, instead of working with $\phi$, they can instead work with $K: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ where $K$ takes two points as input and returns a real value that represents $⟨\phi(x_i), \phi(x_j)⟩$. Note that $\phi$ exists only when $K$ is positive definite. With this, we are able to run SVM on an infinite dimensional space.</p>
<p><strong>Gram Matrix:</strong> Given a set of vectors in $\mathcal{V}$, the Gram Matrix is the matrix of all possible inner products in $\mathcal{V}$. That is, $G_{ij} = v_i \cdot v_j$.</p>
<h3 id="curse-of-dimensionality">Curse of Dimensionality<a hidden class="anchor" aria-hidden="true" href="#curse-of-dimensionality">#</a></h3>
<p>As the dimensionality increases, the classifier&rsquo;s performance increases until the optimal number of features is reached. Further increasing the dimensionality without increasing the number of training samples results in a decrease in classifier performance. The common theme of these problems is that when the dimensionality increases, the volume of the space increases so fast that the available data become sparse. In order to obtain a statistically sound and reliable result, the amount of data needed to support the result often grows exponentially with the dimensionality. Also, the time complexity of a classification algorithm is proportional to the dimension of the data point. So, higher dimension means larger time complexity (not to mention space complexity to store those large dimensional points).</p>
<p><strong>Mercer&rsquo;s Theorem (simplified idea):</strong> In a finite input space, if the Kernel matrix $\mathbf{K}$ (also known as Gram matrix) is positive semi-definite ($\mathbf{K}_{ij} = K(x_i, x_j) = \phi(x_i) \cdot \phi(x_j)$), then the matrix element, <em>i.e.</em> the function K, can be a kernel function.</p>
<h3 id="example-of-a-kernel-function">Example of a kernel function<a hidden class="anchor" aria-hidden="true" href="#example-of-a-kernel-function">#</a></h3>
<p>Let $x_i = (x_{i1}, x_{i2}), x_j = (x_{j1}, x_{j2})$ and $\phi(x_i)=(x_{i1}^2, \sqrt{2}x_{i1}x_{i2}, x_{i2}^2), \phi(x_j)=(x_{j1}^2, \sqrt{2}x_{j1}x_{j2}, x_{j2}^2)$. Then</p>
<img src="https://img-blog.csdnimg.cn/20200328003446513.png" width="500">
<p>From this example, we see that even if the dimension of the feature space is higher than the input space, we can still do the computation in the low dimension input space as long as we choose a good kernel! <strong>Therefore, it&rsquo;s possible to run SVM on an infinite dimensional feature space but do the same amount of computation as in the low dimension input space if we choose a good kernel.</strong></p>
<p><strong>Note:</strong></p>
<ul>
<li><strong>Let $n$ be the dimension of input space, $N (&raquo; n)$ be the dimension of the feature space, then if we choose a kernel $K$ properly, we are able to compute the dot product in higher dimensional space but in complexity $O(n)$ instead of $O(N)$.</strong></li>
<li><strong>If the classification algorithm is only dependent on the dot product and has no dependency on the actual map $\phi$, I can use the kernel trick to run the algorithm in high dimensional space with almost no additional cost.</strong></li>
</ul>
<h2 id="common-kernel-function">Common Kernel Function<a hidden class="anchor" aria-hidden="true" href="#common-kernel-function">#</a></h2>
<h3 id="polynomial-kernel">Polynomial Kernel<a hidden class="anchor" aria-hidden="true" href="#polynomial-kernel">#</a></h3>
<p>$$k(x_i, x_j) = (x_i \cdot x_j + 1)^d$$</p>
<p>where $d$ is the degree of the polynomial. This type of kernel represents the similarity of vectors in a feature space over polynomials of the original variables. It is popular in natural language processing.</p>
<h3 id="gaussian-kernel">Gaussian Kernel<a hidden class="anchor" aria-hidden="true" href="#gaussian-kernel">#</a></h3>
<p>$$k(x, y) = \text{exp}\left(-\frac{|x_i - x_j|^2}{2\sigma^2}\right)$$</p>
<p>This type of kernel is useful when there is no prior knowledge about the data; it has good performance when there is the assumption og general smoothness of the data. It is an example of the radial basis function kernel (below). $\sigma$ is the regularization variable that can be tuned specifically for each problem.</p>
<h3 id="gaussian-radial-basis-function-rbf">Gaussian Radial Basis Function (RBF)<a hidden class="anchor" aria-hidden="true" href="#gaussian-radial-basis-function-rbf">#</a></h3>
<p>$$k(x_i, x_j) = \text{exp}(-\gamma |x_i - x_j|^2)$$</p>
<p>for $\gamma &gt; 0$. The difference between this kernel and the gaussian kernel is the amount of regularization applied.</p>
<h3 id="exponential-kernel">Exponential Kernel<a hidden class="anchor" aria-hidden="true" href="#exponential-kernel">#</a></h3>
<p>$$k(x, y) = \text{exp}\left(-\frac{|x_i - x_j|}{2\sigma^2}\right)$$</p>
<h2 id="dual-problem-of-svm">Dual problem of SVM<a hidden class="anchor" aria-hidden="true" href="#dual-problem-of-svm">#</a></h2>
<p>Our primal problem is</p>
<img src="https://img-blog.csdnimg.cn/20200328003834999.png" width="500">
<p>Note that the primal problem of SVM is a convex problem and the constraints are convex. We know that for any convex optimization problem with differentiable objective and constraint functions, any points that satisfy the KKT conditions are primal and dual optimal, and have zero duality gap.</p>
<p>We can re-write the primal problem as</p>
<img src="https://img-blog.csdnimg.cn/20200328003905975.png" width="500">
<p>where $\phi(x_i)$ is a map of $x_i$ from input space to feature space. Now</p>
<img src="https://img-blog.csdnimg.cn/20200328003930682.png" width="800">
<p>Then we can use the $4$-th KKT condition (gradient w.r.t. $w, b, \epsilon_i$ is $0$):</p>
<img src="https://img-blog.csdnimg.cn/20200328003947720.png" width="600">
<p>Therefore we have</p>
<img src="https://img-blog.csdnimg.cn/20200328004005674.png" width="800">
<p>Our dual problem of SVM is</p>
<img src="https://img-blog.csdnimg.cn/20200328004036594.png" width="800">
<p>Note that the maximization only depends on the dot product of $\phi(x_i), \phi(x_j)$. We define a function</p>
<p>$$K(x_i, x_j) = \phi(x_i) \cdot \phi(x_j)$$</p>
<p>All we need is the function $K$, a kernel function, which provides with the dot product of two vectors in another space and we don&rsquo;t need to know the transformation into the other space.</p>
<p>We can re-write the problem as</p>
<img src="https://img-blog.csdnimg.cn/20200328004054885.png" width="700">
<hr>
<p>Reference:</p>
<ul>
<li><a href="https://www.quora.com/Why-should-a-kernel-function-satisfy-Mercers-condition">https://www.quora.com/Why-should-a-kernel-function-satisfy-Mercers-condition</a></li>
<li><a href="https://scikit-learn.org/stable/modules/svm.html#svm-kernels">https://scikit-learn.org/stable/modules/svm.html#svm-kernels</a></li>
<li><a href="https://kfrankc.com/posts/2019/06/21/kernel-functions-svm#">https://kfrankc.com/posts/2019/06/21/kernel-functions-svm#</a></li>
<li><a href="https://towardsdatascience.com/understanding-support-vector-machine-part-2-kernel-trick-mercers-theorem-e1e6848c6c4d">https://towardsdatascience.com/understanding-support-vector-machine-part-2-kernel-trick-mercers-theorem-e1e6848c6c4d</a></li>
<li><a href="https://www.datasciencecentral.com/profiles/blogs/about-the-curse-of-dimensionality">https://www.datasciencecentral.com/profiles/blogs/about-the-curse-of-dimensionality</a></li>
<li><a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">https://en.wikipedia.org/wiki/Curse_of_dimensionality</a></li>
<li><a href="https://stats.stackexchange.com/questions/44166/kernelised-k-nearest-neighbour">https://stats.stackexchange.com/questions/44166/kernelised-k-nearest-neighbour</a></li>
<li><a href="https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec13.pdf">https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec13.pdf</a></li>
</ul>


  </div>
  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://Liyan06.github.io/tags/ml/">ML</a></li>
      <li><a href="https://Liyan06.github.io/tags/math/">MATH</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://Liyan06.github.io/posts/kaggle_google_quest/">
    <span class="title">« Prev Page</span>
    <br>
    <span>Kaggle: Google Quest Q&amp;A Labeling - my solution</span>
  </a>
  <a class="next" href="https://Liyan06.github.io/posts/convex2/">
    <span class="title">Next Page »</span>
    <br>
    <span>Introduction to Convex Optimization - Primal problem to Dual problem</span>
  </a>
</nav>

<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share SVM, Dual SVM, Non-linear SVM on twitter"
        href="https://twitter.com/intent/tweet/?text=SVM%2c%20Dual%20SVM%2c%20Non-linear%20SVM&amp;url=https%3a%2f%2fLiyan06.github.io%2fposts%2fsvm%2f&amp;hashtags=ML%2cMATH">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share SVM, Dual SVM, Non-linear SVM on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fLiyan06.github.io%2fposts%2fsvm%2f&amp;title=SVM%2c%20Dual%20SVM%2c%20Non-linear%20SVM&amp;summary=SVM%2c%20Dual%20SVM%2c%20Non-linear%20SVM&amp;source=https%3a%2f%2fLiyan06.github.io%2fposts%2fsvm%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share SVM, Dual SVM, Non-linear SVM on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fLiyan06.github.io%2fposts%2fsvm%2f&title=SVM%2c%20Dual%20SVM%2c%20Non-linear%20SVM">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share SVM, Dual SVM, Non-linear SVM on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fLiyan06.github.io%2fposts%2fsvm%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share SVM, Dual SVM, Non-linear SVM on whatsapp"
        href="https://api.whatsapp.com/send?text=SVM%2c%20Dual%20SVM%2c%20Non-linear%20SVM%20-%20https%3a%2f%2fLiyan06.github.io%2fposts%2fsvm%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share SVM, Dual SVM, Non-linear SVM on telegram"
        href="https://telegram.me/share/url?text=SVM%2c%20Dual%20SVM%2c%20Non-linear%20SVM&amp;url=https%3a%2f%2fLiyan06.github.io%2fposts%2fsvm%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main>
    <footer class="footer">
    <span>&copy; 2021 <a href="https://Liyan06.github.io/">Liyan Tang</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)">
    <button class="top-link" id="top-link" type="button" accesskey="g">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z" />
        </svg>
    </button>
</a>

<script>
    let menu = document.getElementById('menu')
    menu.scrollLeft = localStorage.getItem("menu-scroll-position");
    menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
