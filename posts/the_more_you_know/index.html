<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Paper - The More You Know: Using Knowledge Graphs for Image Classification  | Liyan Tang</title>
<meta name="keywords" content="PAPER, KG, CV" />
<meta name="description" content="Overview Note: This previous post I wrote might be helpful for reading this paper summary:
 Introduction to Graph Neural Network (GNN)  This paper investigates the use of structured prior knowledge in the form of knowledge graphs and shows that using this knowledge improves performance on image classification.
It introduce the Graph Search Neural Network (GSNN) as a way of efficiently incorporating large knowledge graphs into a vision classification pipeline, which outperforms standard neural network baselines for multi-label classification.">
<meta name="author" content="Theme PaperMod">
<link rel="canonical" href="https://Liyan06.github.io/posts/the_more_you_know/" />
<meta name="google-site-verification" content="UA-202974782-1" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.35cd0f65a15cafa92372b8313deef5960aae04b90ad722f2bbf509eb0468137e.css" integrity="sha256-Nc0PZaFcr6kjcrgxPe71lgquBLkK1yLyu/UJ6wRoE34=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://Liyan06.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://Liyan06.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://Liyan06.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://Liyan06.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://Liyan06.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.85.0" />
<script>
MathJax = {
  tex: {
    inlineMath: [ ['$', '$'], ['\\(', '\\)'] ]
  }
  ,svg: {
    fontCache: 'global'
  }
};
</script>
<script
  type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>
<meta property="og:title" content="Paper - The More You Know: Using Knowledge Graphs for Image Classification " />
<meta property="og:description" content="Overview Note: This previous post I wrote might be helpful for reading this paper summary:
 Introduction to Graph Neural Network (GNN)  This paper investigates the use of structured prior knowledge in the form of knowledge graphs and shows that using this knowledge improves performance on image classification.
It introduce the Graph Search Neural Network (GSNN) as a way of efficiently incorporating large knowledge graphs into a vision classification pipeline, which outperforms standard neural network baselines for multi-label classification." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://Liyan06.github.io/posts/the_more_you_know/" /><meta property="og:image" content="https://Liyan06.github.io/papermod-cover.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2020-09-02T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2020-09-02T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://Liyan06.github.io/papermod-cover.png"/>

<meta name="twitter:title" content="Paper - The More You Know: Using Knowledge Graphs for Image Classification "/>
<meta name="twitter:description" content="Overview Note: This previous post I wrote might be helpful for reading this paper summary:
 Introduction to Graph Neural Network (GNN)  This paper investigates the use of structured prior knowledge in the form of knowledge graphs and shows that using this knowledge improves performance on image classification.
It introduce the Graph Search Neural Network (GSNN) as a way of efficiently incorporating large knowledge graphs into a vision classification pipeline, which outperforms standard neural network baselines for multi-label classification."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://Liyan06.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Paper - The More You Know: Using Knowledge Graphs for Image Classification ",
      "item": "https://Liyan06.github.io/posts/the_more_you_know/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Paper - The More You Know: Using Knowledge Graphs for Image Classification ",
  "name": "Paper - The More You Know: Using Knowledge Graphs for Image Classification ",
  "description": "Overview Note: This previous post I wrote might be helpful for reading this paper summary:\n Introduction to Graph Neural Network (GNN)  This paper investigates the use of structured prior knowledge in the form of knowledge graphs and shows that using this knowledge improves performance on image classification.\nIt introduce the Graph Search Neural Network (GSNN) as a way of efficiently incorporating large knowledge graphs into a vision classification pipeline, which outperforms standard neural network baselines for multi-label classification.",
  "keywords": [
    "PAPER", "KG", "CV"
  ],
  "articleBody": "Overview Note: This previous post I wrote might be helpful for reading this paper summary:\n Introduction to Graph Neural Network (GNN)  This paper investigates the use of structured prior knowledge in the form of knowledge graphs and shows that using this knowledge improves performance on image classification.\nIt introduce the Graph Search Neural Network (GSNN) as a way of efficiently incorporating large knowledge graphs into a vision classification pipeline, which outperforms standard neural network baselines for multi-label classification.\nIntuition While modern learning-based approaches can recognize some categories with high accuracy, it usually requires thousands of labeled examples for each of these categories. This approach of building large datasets for every concept is unscalable. One way to solve this problem is to use structured knowledge and reasoning (prior knowledge, this is what human usually do but current approaches do not).\nFor example, when people try to identify the animal shown in the figure, they will first recognize the animal, then recall relevant knowledge, and finally reason about it. With this information, even if we have only seen one or two pictures of this animal, we would be able to classify it. So we hope a model could also have similar reasoning process.\nPrevious Work There has been a lot of work in end-to-end learning on graphs or neural network trained on graphs. Most of these approaches either extract features from the graph or they learn a propagation model that transfers evidence between nodes conditional on the type of edge. An example of this is the Gated Graph Neural Network which takes an arbitrary graph as input. Given some initialization specific to the task, it learns how to propagate information and predict the output for every node in the graph.\nPrevious works are focusing on building and then querying knowledge bases rather than using existing knowledge bases as side information for some vision task.\nThis work not only uses attribute relationships that appear in our knowledge graphs, but also uses relationships between objects and reasons directly on graphs rather than using object-attribute pairs directly.\nMajor Contribution   The introduction of the GSNN as a way of incorporating potentially large knowledge graphs into an end-to-end learning system that is computationally feasible for large graphs;\n  Provide a framework for using noisy knowledge graphs for image classification (In vision problems, graphs encode contextual and common-sense relationships and are significantly larger and noisier);\n  The ability to explain image classifications by using the propagation model (Interpretability).\n  Graph Search Neural Network (GSNN) GSNN Explanation The idea is that rather than performing the recurrent update over all of the nodes of the graph at once, it starts with some initial nodes based on the input and only choose to expand nodes which are useful for the final output. Thus, the model only compute the update steps over a subset of the graph.\nSteps in GSNN:\n Determine Initial Nodes  They determine initial nodes in the graph based on likelihood of the visual concept being present as determined by an object detector or classifier. For their experiments, they use Faster R-CNN for each of the 80 COCO categories. For scores over some chosen threshold, they choose the corresponding nodes in the graph as our initial set of active nodes. Once they have initial nodes, they also add the nodes adjacent to the initial nodes to the active set.\n Propagation  Given the initial nodes, they want to first propagate the beliefs about the initial nodes to all of the adjacent nodes (propagation network). This process is similar to GGNN.\n Decide which nodes to expand next  After the first time step, they need a way of deciding which nodes to expand next. Therefore, a per-node scoring function is learned to estimates how “important” that node is. After each propagation step, for every node in the current graph, the model predict an importance score\n$$ i_{v}^{(t)}=g_{i}\\left(h_{v}, x_{v}\\right) $$\nwhere $g_{i}$ is a learned network, the **importance network**. Once we have values of $i_{v}$, we take the top $P$ scoring nodes that have never been expanded and add them to the expanded set, and add all nodes adjacent to those nodes to our active set.\nThe above two steps (Propagation, Decide which nodes to expand next) repeated $T$ times ($T$ is a hyper-parameter).\nLastly, at the final time step $T$, the model computes the per-node-output (output network) and re-order and zero-pad the outputs into the final classification net. Re-order it so that nodes always appear in the same order into the final network, and zero pad any nodes that were not expanded.\nThe entire process is shown in the figure above.\nThree networks   Propagation network: normal Graph Gated Neural Network (GGNN). GGNN is a fully end-to-end network that takes as input a directed graph and outputs either a classification over the entire graph or an output for each node. Check my previous post to know more details about GGNN and how propagation works.\n  Output network: After $T$ time steps, we have our final hidden states. The node level outputs can then just be computed as $$ o_{v}=g\\left(h_{v}^{(T)}, x_{v}\\right) $$ where $g$ is a fully connected network, the output network, and $x_{v}$ is the original annotation for the node.\n  Importance network: learn a per-node scoring function that estimates how “important” that node is. To train the importance net, they assign target importance value to each node in the graph for a given image. Nodes corresponding to ground-truth concepts in an image are assigned an importance value of 1. The neighbors of these nodes are assigned a value of $\\gamma$. Nodes which are two-hop away have value $\\gamma^2$ and so on. The idea is that nodes closest to the final output are the most important to expand. After each propagation step, for each node in the current graph, they predict an importance score $$ i_{v}^{(t)}=g_{i}\\left(h_{v}, x_{v}\\right) $$\n  Diagram visualization First $x_{init}$, the detection confidences initialize $h_{i n i t}^{(1)}$, the hidden states of the initially detected nodes (Each visual concept (*e.x.*, person, horse, cat, *etc*) in the knowledge graph is represented in a hidden state). We then initialize $h_{a d j 1}^{(1)}$, the hidden states of the adjacent nodes, with $0$. We then update the hidden states using the propagation net. The values of $h^{(2)}$ are then used to predict the importance scores $i^{(1)}$ which are used to pick the next nodes to add $adj2$. These nodes are then initialized with $h_{a d j 2}^{(2)}=0$ and the hidden states are updated again through the propagation net. After $T$ steps, we then take all of the accumulated hidden states $h^{T}$ to predict the GSNN outputs for all the active nodes. During backpropagation, the binary cross entropy (BCE) loss is fed backward through the output layer, and the importance losses are fed through the importance networks to update the network parameters.\nOne final detail is the addition of a “node bias” into GSNN. In GGNN, the per-node output function $g\\left(h_{v}^{(T)}, x_{v}\\right)$, takes in the hidden state and initial annotation of the node $v$ to compute its output. Our output equations are now $g\\left(h_{v}^{(T)}, x_{v}, n_{v}\\right)$ where $n_{v}$ is a bias term that is tied to a particular node $v$ in the overall graph. This value is stored in a table and its value are updated by backpropagation.\nAdvantage   This new architecture mitigates the computational issues with the Gated Graph Neural Networks for large graphs which allows our model to be efficiently trained for image tasks using large knowledge graphs.\n  Importantly, the GSNN model is also able to provide explanations on classifications by following how the information is propagated in the graph.\n  Incorporate the graph network into an image pipeline We take the output of the graph network, re-order it so that nodes always appear in the same order into the final network, and zero pad any nodes that were not expanded. Therefore, if we have a graph with 316 node outputs, and each node predicts a 5-dim hidden variable, we create a 1580-dim feature vector from the graph. We also concatenate this feature vector with fc7 layer (4096-dim) of a fine-tuned VGG-16 network and top-score for each COCO category predicted by Faster R-CNN (80-dim). This 5756-dim feature vector is then fed into 1-layer final classification network trained with dropout.\nYou can think of this as a way of feature engineering where you concatenate the output from models with different structures.\nDataset COCO: COCO is a large-scale object detection, segmentation, and captioning dataset, which includes 80 object categories.\nVisual Genome: a dataset that represents the complex, noisy visual world with its many different kinds of objects, where labels are potentially ambiguous and overlapping, and categories fall into a long-tail distribution (skew to the right).\nVisual Genome contains over 100,000 natural images from the Internet. Each image is labeled with objects, attributes and relationships between objects entered by human annotators. They create a subset from Visual Genome which they call Visual Genome multi-label dataset or VGML (They took 316 visual concepts from the subset).\nUsing only the train split, we build a knowledge graph connecting the concepts using the most common object-attribute and object-object relationships in the dataset. Specifically, we counted how often an object/object relationship or object/attribute pair occurred in the training set, and pruned any edges that had fewer than 200 instances. This leaves us with a graph over all of the images with each edge being a common relationship.\nVisual Genome + WordNet: including the outside semantic knowledge from WordNet. The Visual Genome graphs do not contain useful semantic relationships. For instance, it might be helpful to know that dog is an animal if our visual system sees a dog and one of our labels is animal. Check the original paper to know how to add WordNet into the graph.\nConclusion In this paper, they present the Graph Search Neural Network (GSNN) as a way of efficiently using knowledge graphs as extra information to improve image classification.\nNext Step:\nThe GSNN and the framework they use for vision problems is completely general. The next steps will be to apply the GSNN to other vision tasks, such as detection, Visual Question Answering, and image captioning.\nAnother interesting direction would be to combine the procedure of this work with a system such as NEIL to create a system which builds knowledge graphs and then prunes them to get a more accurate, useful graph for image tasks.\n Reference:\n Visual Genome: https://visualgenome.org The More You Know: Using Knowledge Graphs for Image Classification: https://arxiv.org/abs/1612.04844  ",
  "wordCount" : "1740",
  "inLanguage": "en",
  "datePublished": "2020-09-02T00:00:00Z",
  "dateModified": "2020-09-02T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Theme PaperMod"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://Liyan06.github.io/posts/the_more_you_know/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Liyan Tang",
    "logo": {
      "@type": "ImageObject",
      "url": "https://Liyan06.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<noscript>
    <style type="text/css">
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: #1d1e20;
                --entry: #2e2e33;
                --primary: rgba(255, 255, 255, 0.84);
                --secondary: rgba(255, 255, 255, 0.56);
                --tertiary: rgba(255, 255, 255, 0.16);
                --content: rgba(255, 255, 255, 0.74);
                --hljs-bg: #2e2e33;
                --code-bg: #37383e;
                --border: #333;
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://Liyan06.github.io/" accesskey="h" title="Liyan Tang (Alt + H)">Liyan Tang</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://Liyan06.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://Liyan06.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://Liyan06.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://Liyan06.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://Liyan06.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://Liyan06.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      Paper - The More You Know: Using Knowledge Graphs for Image Classification 
    </h1>
    <div class="post-meta">September 2, 2020&nbsp;·&nbsp;9 min&nbsp;·&nbsp;Theme PaperMod
</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <div class="details">Table of Contents</div>
        </summary>
        <div class="inner"><ul>
                <li>
                    <a href="#overview" aria-label="Overview">Overview</a></li>
                <li>
                    <a href="#intuition" aria-label="Intuition">Intuition</a></li>
                <li>
                    <a href="#previous-work" aria-label="Previous Work">Previous Work</a></li>
                <li>
                    <a href="#major-contribution" aria-label="Major Contribution">Major Contribution</a></li>
                <li>
                    <a href="#graph-search-neural-network-gsnn" aria-label="Graph Search Neural Network (GSNN)">Graph Search Neural Network (GSNN)</a><ul>
                        
                <li>
                    <a href="#gsnn-explanation" aria-label="GSNN Explanation">GSNN Explanation</a></li>
                <li>
                    <a href="#three-networks" aria-label="Three networks">Three networks</a></li>
                <li>
                    <a href="#diagram-visualization" aria-label="Diagram visualization">Diagram visualization</a></li>
                <li>
                    <a href="#advantage" aria-label="Advantage">Advantage</a></li></ul>
                </li>
                <li>
                    <a href="#incorporate-the-graph-network-into-an-image-pipeline" aria-label="Incorporate the graph network into an image pipeline">Incorporate the graph network into an image pipeline</a></li>
                <li>
                    <a href="#dataset" aria-label="Dataset">Dataset</a></li>
                <li>
                    <a href="#conclusion" aria-label="Conclusion">Conclusion</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="overview">Overview<a hidden class="anchor" aria-hidden="true" href="#overview">#</a></h2>
<p>Note: This previous post I wrote might be helpful for reading this paper summary:</p>
<ul>
<li><a href="./GNN.md">Introduction to Graph Neural Network (GNN)</a></li>
</ul>
<p>This paper investigates <strong>the use of structured prior knowledge in the form of knowledge graphs</strong> and shows that using this knowledge improves performance on image classification.</p>
<p>It introduce the <em>Graph Search Neural Network (GSNN)</em> as a way of efficiently incorporating large knowledge graphs into a vision classification pipeline, which outperforms standard neural network baselines for multi-label classification.</p>
<h2 id="intuition">Intuition<a hidden class="anchor" aria-hidden="true" href="#intuition">#</a></h2>
<p>While modern learning-based approaches can recognize some categories with high accuracy, it usually requires thousands of labeled examples for each of these categories. This approach of building large datasets for every concept is unscalable. One way to solve this problem is to use structured knowledge and reasoning (prior knowledge, this is what human usually do but current approaches do not).</p>
<img src="https://img-blog.csdnimg.cn/20200901235037534.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_1,color_FFFFFF,t_70#pic_center" width=550>
<p>For example, when people try to identify the animal shown in the figure, they will first <em>recognize</em> the animal, then <em>recall relevant knowledge</em>, and finally <em>reason</em> about it. With this information, even if we have only seen one or two pictures of this animal, we would be able to classify it. So we hope a model could also have similar reasoning process.</p>
<h2 id="previous-work">Previous Work<a hidden class="anchor" aria-hidden="true" href="#previous-work">#</a></h2>
<p>There has been a lot of work in end-to-end learning on graphs or neural network trained on graphs. Most of these approaches either extract features from the graph or they learn a propagation model that transfers evidence between nodes conditional on the type of edge. An example of this is the Gated Graph Neural Network which takes an arbitrary graph as input. Given some initialization specific to the task, it learns how to propagate information and predict the output for every node in the graph.</p>
<img src="https://img-blog.csdnimg.cn/20200901235137987.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_1,color_FFFFFF,t_70#pic_center" width=550>
<p>Previous works are focusing on building and then querying knowledge bases <strong>rather than using existing knowledge bases as side information</strong> for some vision task.</p>
<p><strong>This work not only uses attribute relationships that appear in our knowledge graphs, but also uses relationships between objects and reasons directly on graphs rather than using object-attribute pairs directly.</strong></p>
<h2 id="major-contribution">Major Contribution<a hidden class="anchor" aria-hidden="true" href="#major-contribution">#</a></h2>
<ol>
<li>
<p>The introduction of the GSNN as a way of incorporating potentially large knowledge graphs into an end-to-end learning system that is computationally feasible for large graphs;</p>
</li>
<li>
<p>Provide a framework for using noisy knowledge graphs for image classification (In vision problems, graphs encode contextual and common-sense relationships and are significantly larger and noisier);</p>
</li>
<li>
<p>The ability to explain image classifications by using the propagation model (Interpretability).</p>
</li>
</ol>
<h2 id="graph-search-neural-network-gsnn">Graph Search Neural Network (GSNN)<a hidden class="anchor" aria-hidden="true" href="#graph-search-neural-network-gsnn">#</a></h2>
<h3 id="gsnn-explanation">GSNN Explanation<a hidden class="anchor" aria-hidden="true" href="#gsnn-explanation">#</a></h3>
<p>The idea is that rather than performing the recurrent update over all of the nodes of the graph at once, it starts with some initial nodes based on the input and only choose to expand nodes which are useful for the final output. Thus, the model only compute the update steps over a subset of the graph.</p>
<img src="https://img-blog.csdnimg.cn/20200901235256289.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_1,color_FFFFFF,t_70#pic_center" width=770>
<p><strong>Steps in GSNN:</strong></p>
<ul>
<li>Determine Initial Nodes</li>
</ul>
<p>They determine initial nodes in the graph based on likelihood of the visual concept being present as determined by an object detector or classifier. For their experiments, they use Faster R-CNN for each of the 80 COCO categories. For scores over some chosen threshold, they choose the corresponding nodes in the graph as our initial set of active nodes. Once they have initial nodes, they also add the nodes adjacent to the initial nodes to the active set.</p>
<ul>
<li><em>Propagation</em></li>
</ul>
<p>Given the initial nodes, they want to first propagate the beliefs about the initial nodes to all of the adjacent nodes (<strong>propagation network</strong>). This process is similar to GGNN.</p>
<ul>
<li><em>Decide which nodes to expand next</em></li>
</ul>
<p>After the first time step, they need a way of deciding which nodes to expand next. Therefore, a per-node scoring function is learned to estimates how &ldquo;important&rdquo; that node is. After each propagation step, for every node in the current graph, the model predict an importance score</p>
<p>$$
i_{v}^{(t)}=g_{i}\left(h_{v}, x_{v}\right)
$$</p>
<p>where $g_{i}$ is a learned network, the **importance network**. Once we have values of $i_{v}$, we take the top $P$ scoring nodes that have never been expanded and add them to the expanded set, and add all nodes adjacent to those nodes to our active set.</p>
<p>The above two steps (Propagation, Decide which nodes to expand next) repeated $T$ times ($T$ is a hyper-parameter).</p>
<p>Lastly, at the final time step $T$, the model computes the per-node-output (<strong>output network</strong>) and re-order and zero-pad the outputs into the final classification net. <strong>Re-order it so that nodes always appear in the same order into the final network, and zero pad any nodes that were not expanded.</strong></p>
<p>The entire process is shown in the figure above.</p>
<h3 id="three-networks">Three networks<a hidden class="anchor" aria-hidden="true" href="#three-networks">#</a></h3>
<ul>
<li>
<p><strong>Propagation network</strong>: normal Graph Gated Neural Network (GGNN). GGNN is a fully end-to-end network that takes as input a directed graph and outputs either a classification over the entire graph or an output for each node. Check my previous <a href="https://liyantang.blog.csdn.net/article/details/108047975">post</a> to know more details about GGNN and how propagation works.</p>
</li>
<li>
<p><strong>Output network</strong>: After $T$ time steps, we have our final hidden states. The node level outputs can then just be computed as
$$
o_{v}=g\left(h_{v}^{(T)}, x_{v}\right)
$$
where $g$ is a fully connected network, the output network, and $x_{v}$ is the original annotation for the node.</p>
</li>
<li>
<p><strong>Importance network</strong>: learn a per-node scoring function that estimates how &ldquo;important&rdquo; that node is. To train the importance net, they <em>assign target importance value to each node in the graph for a given image</em>. Nodes corresponding to ground-truth concepts in an image are assigned an importance value of 1. The neighbors of these nodes are assigned a value of $\gamma$. Nodes which are two-hop away have value $\gamma^2$ and so on. The idea is that nodes closest to the final output are the most important to expand. After each propagation step, for each node in the current graph, they predict an importance score
$$
i_{v}^{(t)}=g_{i}\left(h_{v}, x_{v}\right)
$$</p>
</li>
</ul>
<h3 id="diagram-visualization">Diagram visualization<a hidden class="anchor" aria-hidden="true" href="#diagram-visualization">#</a></h3>
<img src="https://img-blog.csdnimg.cn/20200901235357743.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_16,color_FFFFFF,t_70#pic_center" width=600>
<p>First $x_{init}$, the detection confidences initialize $h_{i n i t}^{(1)}$, the hidden states of the initially detected nodes (Each visual concept (*e.x.*, person, horse, cat, *etc*) in the knowledge graph is represented in a hidden state). We then initialize $h_{a d j 1}^{(1)}$, the hidden states of the adjacent nodes, with $0$. We then update the hidden states using the propagation net. The values of $h^{(2)}$ are then used to predict the importance scores $i^{(1)}$ which are used to pick the next nodes to add $adj2$. These nodes are then initialized with $h_{a d j 2}^{(2)}=0$ and the hidden states are updated again through the propagation net. After $T$ steps, we then take all of the accumulated hidden states $h^{T}$ to predict the GSNN outputs for all the active nodes. During backpropagation, the binary cross entropy (BCE) loss is fed backward through the output layer, and the importance losses are fed through the importance networks to update the network parameters.</p>
<p>One final detail is the addition of a &ldquo;node bias&rdquo; into GSNN. In GGNN, the per-node output function $g\left(h_{v}^{(T)}, x_{v}\right)$, takes in the hidden state and initial annotation of the node $v$ to compute its output. Our output equations are now $g\left(h_{v}^{(T)}, x_{v}, n_{v}\right)$ where $n_{v}$ is a bias term that is tied to a particular node $v$ in the overall graph. This value is stored in a table and its value are updated by backpropagation.</p>
<h3 id="advantage">Advantage<a hidden class="anchor" aria-hidden="true" href="#advantage">#</a></h3>
<ul>
<li>
<p>This new architecture mitigates the computational issues with the Gated Graph Neural Networks for large graphs which allows our model to be efficiently trained for image tasks using large knowledge graphs.</p>
</li>
<li>
<p>Importantly, the GSNN model is also able to <strong>provide explanations on classifications</strong> by following how the information is propagated in the graph.</p>
</li>
</ul>
<h2 id="incorporate-the-graph-network-into-an-image-pipeline">Incorporate the graph network into an image pipeline<a hidden class="anchor" aria-hidden="true" href="#incorporate-the-graph-network-into-an-image-pipeline">#</a></h2>
<p>We take the output of the graph network, <strong>re-order it so that nodes always appear in the same order into the final network, and zero pad any nodes that were not expanded.</strong> Therefore, if we have a graph with 316 node outputs, and each node predicts a 5-dim hidden variable, we create a 1580-dim feature vector from the graph. We also concatenate this feature vector with fc7 layer (4096-dim) of a fine-tuned VGG-16 network and top-score for each COCO category predicted by Faster R-CNN (80-dim). This 5756-dim feature vector is then fed into 1-layer final classification network trained with dropout.</p>
<p>You can think of this as a way of feature engineering where you concatenate the output from models with different structures.</p>
<img src="https://img-blog.csdnimg.cn/20200901235445426.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_1,color_FFFFFF,t_70#pic_center" width=650>
<h2 id="dataset">Dataset<a hidden class="anchor" aria-hidden="true" href="#dataset">#</a></h2>
<p>COCO: COCO is a large-scale object detection, segmentation, and captioning dataset, which includes 80 object categories.</p>
<p>Visual Genome: a dataset that represents the complex, noisy visual world with its many different kinds of objects, where labels are potentially ambiguous and overlapping, and categories fall into a long-tail distribution (skew to the right).</p>
<img src="https://img-blog.csdnimg.cn/20200902060912766.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_1,color_FFFFFF,t_70#pic_center" width=650>
<p>Visual Genome contains over 100,000 natural images from the Internet. Each image is labeled with objects, attributes and relationships between objects entered by human annotators. They create a subset from Visual Genome which they call Visual Genome multi-label dataset or VGML (They took 316 visual concepts from the subset).</p>
<p><em>Using only the train split, we build a knowledge graph connecting the concepts using the most common object-attribute and object-object relationships in the dataset. Specifically, we counted how often an object/object relationship or object/attribute pair occurred in the training set, and pruned any edges that had fewer than 200 instances. This leaves us with a graph over all of the images with each edge being a common relationship.</em></p>
<p>Visual Genome + WordNet: including the <em>outside semantic knowledge</em> from WordNet. The Visual Genome graphs do not contain useful semantic relationships. For instance, it might be helpful to know that dog is an animal if our visual system sees a dog and one of our labels is animal. Check the original paper to know how to add WordNet into the graph.</p>
<img src="https://img-blog.csdnimg.cn/20200901235652262.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0pheV9UYW5n,size_1,color_FFFFFF,t_70#pic_center" width=600>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>In this paper, they present the Graph Search Neural Network (GSNN) as a way of efficiently using knowledge graphs as extra information to improve image classification.</p>
<p>Next Step:</p>
<p>The GSNN and the framework they use for vision problems is completely general. The next steps will be to apply the GSNN to other vision tasks, such as detection, Visual Question Answering, and image captioning.</p>
<p>Another interesting direction would be to combine the procedure of this work with a system such as NEIL <em>to create a system which builds knowledge graphs and then prunes them to get a more accurate, useful graph for image tasks</em>.</p>
<hr>
<p>Reference:</p>
<ul>
<li>Visual Genome: <a href="https://visualgenome.org">https://visualgenome.org</a></li>
<li>The More You Know: Using Knowledge Graphs for Image Classification: <a href="https://arxiv.org/abs/1612.04844">https://arxiv.org/abs/1612.04844</a></li>
</ul>


  </div>
  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://Liyan06.github.io/tags/paper/">PAPER</a></li>
      <li><a href="https://Liyan06.github.io/tags/kg/">KG</a></li>
      <li><a href="https://Liyan06.github.io/tags/cv/">CV</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://Liyan06.github.io/posts/bert_attn/">
    <span class="title">« Prev Page</span>
    <br>
    <span>What Does BERT Look At? An Analysis of BERT’s Attention</span>
  </a>
  <a class="next" href="https://Liyan06.github.io/posts/spectral_conv/">
    <span class="title">Next Page »</span>
    <br>
    <span>Graph Convolutional Neural Network -  Spectral Convolution</span>
  </a>
</nav>

<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Paper - The More You Know: Using Knowledge Graphs for Image Classification  on twitter"
        href="https://twitter.com/intent/tweet/?text=Paper%20-%20The%20More%20You%20Know%3a%20Using%20Knowledge%20Graphs%20for%20Image%20Classification%20&amp;url=https%3a%2f%2fLiyan06.github.io%2fposts%2fthe_more_you_know%2f&amp;hashtags=PAPER%2cKG%2cCV">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Paper - The More You Know: Using Knowledge Graphs for Image Classification  on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fLiyan06.github.io%2fposts%2fthe_more_you_know%2f&amp;title=Paper%20-%20The%20More%20You%20Know%3a%20Using%20Knowledge%20Graphs%20for%20Image%20Classification%20&amp;summary=Paper%20-%20The%20More%20You%20Know%3a%20Using%20Knowledge%20Graphs%20for%20Image%20Classification%20&amp;source=https%3a%2f%2fLiyan06.github.io%2fposts%2fthe_more_you_know%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Paper - The More You Know: Using Knowledge Graphs for Image Classification  on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fLiyan06.github.io%2fposts%2fthe_more_you_know%2f&title=Paper%20-%20The%20More%20You%20Know%3a%20Using%20Knowledge%20Graphs%20for%20Image%20Classification%20">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Paper - The More You Know: Using Knowledge Graphs for Image Classification  on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fLiyan06.github.io%2fposts%2fthe_more_you_know%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Paper - The More You Know: Using Knowledge Graphs for Image Classification  on whatsapp"
        href="https://api.whatsapp.com/send?text=Paper%20-%20The%20More%20You%20Know%3a%20Using%20Knowledge%20Graphs%20for%20Image%20Classification%20%20-%20https%3a%2f%2fLiyan06.github.io%2fposts%2fthe_more_you_know%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Paper - The More You Know: Using Knowledge Graphs for Image Classification  on telegram"
        href="https://telegram.me/share/url?text=Paper%20-%20The%20More%20You%20Know%3a%20Using%20Knowledge%20Graphs%20for%20Image%20Classification%20&amp;url=https%3a%2f%2fLiyan06.github.io%2fposts%2fthe_more_you_know%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main>
    <footer class="footer">
    <span>&copy; 2021 <a href="https://Liyan06.github.io/">Liyan Tang</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)">
    <button class="top-link" id="top-link" type="button" accesskey="g">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z" />
        </svg>
    </button>
</a>

<script>
    let menu = document.getElementById('menu')
    menu.scrollLeft = localStorage.getItem("menu-scroll-position");
    menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
